{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Two Service Deployment Patterns\n",
    "\n",
    "This is a Jupyter notebook.  See http://jupyter.org/ for more information.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Below we compare two service deployment patterns by using discrete-event simulations. Ideally the reader will have had some prior exposure to the Python language in order to follow along all the details.  However, the concepts and conclusions should be understandable to readers with software architecture or engineering background even if not directly familiar with Python.\n",
    "\n",
    "We assume an application made up of multiple multi-threaded services and consider two deployment patterns:\n",
    "\n",
    "- **Monolithic deployment**, where all services making up an application are deployed together on each VM or container\n",
    "- **Microservices deployment**, where each of the services is deployed on its own VM or (more likely) it own container.\n",
    "\n",
    "In the simulations below, the application is made up of just two services, to simplify the model and the analysis, but without loss of generality in terms of the main conclusions.\n",
    "\n",
    "The simulation models are written in the **Python** language.  The choice of Python was based not only on Python's rapid development dynamic language characteristics, but also on the availability libraries well-suited for this kind of work:\n",
    "\n",
    "- **SimPy**, a powerful and easy-to-use simulation framework\n",
    "- **SciPy**, a powerful set of libraries for efficient data analysis and visualization, including *NumPy*, *Pandas*, and *Matplotlib*.\n",
    "\n",
    "The simulations here use a small custom framwork named **ServerSim**, based on *SimPy*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ServerSim core concepts\n",
    "\n",
    "ServerSim consists of a several classes and utilities.  The classes relevant to the simulations in this document are described below.\n",
    "\n",
    "### Server\n",
    "\n",
    "Represents a server -- physical, VM, or container, with a predetermined computation capacity.  A server can execute arbitrary service request types.  The computation capacity of a server is represented in terms of a number of hardware threads and a total speed number (computation units processed per unit of time).  The total speed is equally apportioned among the hardware threads, to give the speed per hardware thread.  A server also has a number of associated software threads (which must be no smaller than the number of hardware threads).  Software threads are relevant for blocking computations only.\n",
    "\n",
    "The simulations in this document assume non-blocking services, so the software threads will not be of consequence.\n",
    "\n",
    "Attributes:\n",
    "- env: The SimPy Environment (required to run SimPy simulations)\n",
    "- maxConcurrency: The maximum of number of hardware threads for the server.\n",
    "- numThreads: The maximum number of software threads for the server.\n",
    "- speed: Aggregate server speed across all hardware threads.\n",
    "- name: The server's name.\n",
    "\n",
    "### SvcRequest\n",
    "\n",
    "Represents a service request to a server.\n",
    "\n",
    "    def __init__(self, env, svcName, fgen, server, inVal, inBlockingCall=False):\n",
    "    Attributes:\n",
    "        inBlockingCall (bool): Indicates whether this request is\n",
    "            in the scope of a blocking call.  When this parameter\n",
    "            is true, the service request will hold a software\n",
    "            thread on the target server while the service\n",
    "            request itself and any of its sub-requests (calls\n",
    "            to other servers) are executing.  Otherwise, the\n",
    "            call is non-blocking, so a thread is held on the\n",
    "            target server only while the service request itself\n",
    "            is executing; the thread is relinquished when\n",
    "            this request finishes executing and it passes contol\n",
    "            to its sub-requests.\n",
    "\n",
    "### SvcRequester\n",
    "\n",
    "### CoreSvcRequester\n",
    "\n",
    "### UserGroup\n",
    "\n",
    "### Other service requester classes\n",
    "\n",
    "There are other service requester classes (subclasses of SvcRequester) in addition to CoreSvcRequester, to define more complex services, including blocking services, asynchronous fire-and-forget services, sequentially dependednt services, parallel service calls, and service continuations.  These additional classes are not used in the simulations in this document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment set-up\n",
    "\n",
    "The code involved in these simulations should be compatible with both Python 2.7 and Python 3.x.\n",
    "\n",
    "Python\n",
    "and the following Python packages need to be installed in your computer:\n",
    "- jupyter-notebook\n",
    "- simpy\n",
    "- numpy\n",
    "- pandas\n",
    "- matplotlib\n",
    "- seaborn\n",
    "\n",
    "If you already have Python and *pip* installed, you can install the above packages with *pip*.  Otherwise, the easiest way to have all of this installed (including Python) is to install [Anaconda](https://www.continuum.io/downloads) or [Miniconda](https://conda.io/miniconda.html).  \n",
    "\n",
    "Anaconda is a big install that includes all of the above and much more stuff for data science and scientific computing.  Miniconda installs Python and a package manager that makes it easy to install Jupyter Notebook and the other packages with the command:\n",
    ">`conda install numpy pandas matplotlib seaborn jupyter-notebook`.\n",
    "\n",
    "The model in this document should be run from the parent directory of the `serversim` package directory, which in turn contains the source files for the ServerSim framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The core simulation function\n",
    "\n",
    "Following is the source code for the core function used in the simulations  This function will be called with different arguments to simulate different scenarios.\n",
    "\n",
    "#### Imports\n",
    "\n",
    "We start by importing the required libraries, as well as the *__future__* import for compatibility between Python 2.7 and Python 3.x.\n",
    "\n",
    "#### Parameters:\n",
    "\n",
    "- *numUsers*: the number of users being simulated.  This parameter can be either a positive integer or a list of pairs.  In the second case, the list of pairs represents a number of users that varies over time as a step function.  The first elements of the pairs in the list must be strictly monotonically increasing and each pair in the list represents a step in the step function.  Each step starts (inclusive) at the time represented by the first component of the corresponding pair and ends (exclusive) at the time represented by the first component of the next pair in the list.\n",
    "- *weight1*: the relative frequency of service requests for the first service.\n",
    "- *weight2*: the relative frequency of service requests for the second service.\n",
    "- *serverRange1*: a Python range representing the numeric server IDs of the servers on which the first service can be deployed.\n",
    "- *serverRange2*: a Python range representing the numeric server IDs of the servers on which the second service can be deployed.  This and the above range can be overlapping.  In case they are overlapping, the servers in the intersection of the ranges will host both the first and the second service.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load deployment_example.py\n",
    "# %%writefile deployment_example.py\n",
    "# We start by importing the required libraries, as well as the __future__\n",
    "# import for compatibility between Python 2.7 and Python 3.x.\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import random\n",
    "import simpy\n",
    "from serversim import *\n",
    "\n",
    "\n",
    "def deployment_example(numUsers, weight1, weight2, serverRange1, serverRange2):\n",
    "\n",
    "    def cug(mid, delta):\n",
    "        \"\"\"Computation units generator\"\"\"\n",
    "        def f():\n",
    "            return random.uniform(mid - delta, mid + delta)\n",
    "        return f\n",
    "\n",
    "    def ldBal(svcType):\n",
    "        \"\"\"Application server load-balancer.\"\"\"\n",
    "        if svcType == \"svc_1\":\n",
    "            server = random.choice(servers1)\n",
    "        elif svcType == \"svc_2\":\n",
    "            server = random.choice(servers2)\n",
    "        else:\n",
    "            assert False, \"Invalid service type.\"\n",
    "        return server\n",
    "\n",
    "    # numUsers = 700\n",
    "    simtime = 200\n",
    "    hwThreads = 10\n",
    "    swThreads = 20\n",
    "    speed = 20\n",
    "    svc_1_comp_units = 2.0\n",
    "    svc_2_comp_units = 1.0\n",
    "\n",
    "    env = simpy.Environment()\n",
    "\n",
    "    nServers = max(serverRange1[-1]+1, serverRange2[-1]+1)\n",
    "    servers = [Server(env, hwThreads, swThreads, speed, \"AppServer_%s\" % i) for i in range(nServers)]\n",
    "    servers1 = [servers[i] for i in serverRange1]\n",
    "    servers2 = [servers[i] for i in serverRange2]\n",
    "\n",
    "    svc_1 = CoreSvcRequester(env, \"svc_1\", cug(svc_1_comp_units, svc_1_comp_units*.9), ldBal)\n",
    "    svc_2 = CoreSvcRequester(env, \"svc_2\", cug(svc_2_comp_units, svc_2_comp_units*.9), ldBal)\n",
    "\n",
    "    weightedTxns = [(svc_1, weight1),\n",
    "                    (svc_2, weight2)\n",
    "                   ]\n",
    "\n",
    "    minThinkTime = 2.0 # .5 # 4\n",
    "    maxThinkTime = 10.0 # 1.5 # 20\n",
    "\n",
    "    grp = UserGroup(env, numUsers, \"UserTypeX\", weightedTxns, minThinkTime, maxThinkTime)\n",
    "    grp.activateUsers()\n",
    "\n",
    "    env.run(until=simtime)\n",
    "    \n",
    "    return {\"numUsers\":numUsers, \"weight1\":weight1, \"weight2\":weight2, \n",
    "            \"serverRange1\":serverRange1, \"serverRange2\":serverRange2, \n",
    "            \"servers\":servers, \"grp\":grp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the simulation results\n",
    "\n",
    "The following function prints the outputs from the above core simulation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load print_results.py\n",
    "# %%writefile print_results.py\n",
    "def print_results(numUsers=None, weight1=None, weight2=None, serverRange1=None,\n",
    "                  serverRange2=None, servers=None, grp=None, fi=None):\n",
    "    \n",
    "    if fi is None:\n",
    "        import sys\n",
    "        fi = sys.stdout\n",
    "\n",
    "    print(\"\\n\\n***** Start Simulation --\", numUsers, \",\", weight1, \",\", weight2, \", [\", serverRange1[0], \",\", serverRange1[-1] + 1,\n",
    "          \") , [\", serverRange2[0], \",\", serverRange2[-1] + 1, \") *****\", file=fi)\n",
    "    print(\"Simulation: numUsers =\", numUsers, file=fi)\n",
    "\n",
    "    print(\"<< ServerExample >>\\n\", file=fi)\n",
    "\n",
    "    indent = \" \" * 4\n",
    "\n",
    "    print(\"\\n\" + \"Servers:\", file=fi)\n",
    "    for svr in servers:\n",
    "        print(indent*1 + \"Server:\", svr.name, file=fi)\n",
    "        print(indent*2 + \"maxConcurrency =\", svr.maxConcurrency, file=fi)\n",
    "        print(indent*2 + \"numThreads =\", svr.numThreads, file=fi)\n",
    "        print(indent*2 + \"speed =\", svr.speed, file=fi)\n",
    "        print(indent*2 + \"avgProcessTime =\", svr.avgProcessTime, file=fi)\n",
    "        print(indent*2 + \"avgHwQueueTime =\", svr.avgHwQueueTime, file=fi)\n",
    "        print(indent*2 + \"avgThreadQueueTime =\", svr.avgThreadQueueTime, file=fi)\n",
    "        print(indent*2 + \"avgServiceTime =\", svr.avgServiceTime, file=fi)\n",
    "        print(indent*2 + \"avgHwQueueLength =\", svr.avgHwQueueLength, file=fi)\n",
    "        print(indent*2 + \"avgThreadQueueLength =\", svr.avgThreadQueueLength, file=fi)\n",
    "        print(indent*2 + \"hwQueueLength =\", svr.hwQueueLength, file=fi)\n",
    "        print(indent*2 + \"hwInProcessCount =\", svr.hwInProcessCount, file=fi)\n",
    "        print(indent*2 + \"threadQueueLength =\", svr.threadQueueLength, file=fi)\n",
    "        print(indent*2 + \"threadInUseCount =\", svr.threadInUseCount, file=fi)\n",
    "        print(indent*2 + \"utilization =\", svr.utilization, file=fi)\n",
    "        print(indent*2 + \"throughput =\", svr.throughput, file=fi)\n",
    "\n",
    "    print(indent*1 + \"Group:\", grp.name, file=fi)\n",
    "    print(indent*2 + \"numUsers =\", grp.numUsers, file=fi)\n",
    "    print(indent*2 + \"minThinkTime =\", grp.minThinkTime, file=fi)\n",
    "    print(indent*2 + \"maxThinkTime =\", grp.maxThinkTime, file=fi)\n",
    "    print(indent*2 + \"respondedRequestCount =\", grp.respondedRequestCount(None), file=fi)\n",
    "    print(indent*2 + \"unrespondedRequestCount =\", grp.unrespondedRequestCount(None), file=fi)\n",
    "    print(indent*2 + \"avgResponseTime =\", grp.avgResponseTime(None), file=fi)\n",
    "    print(indent*2 + \"stdDevResponseTime =\", grp.stdDevResponseTime(None), file=fi)\n",
    "    print(indent*2 + \"throughput =\", grp.throughput(None), file=fi)\n",
    "\n",
    "    for txn in grp._txns:\n",
    "        print(indent*2 + txn.svcName + \":\", file=fi)\n",
    "        print(indent*3 + \"respondedRequestCount =\", grp.respondedRequestCount(txn), file=fi)\n",
    "        print(indent*3 + \"unrespondedRequestCount =\", grp.unrespondedRequestCount(txn), file=fi)\n",
    "        print(indent*3 + \"avgResponseTime =\", grp.avgResponseTime(txn), file=fi)\n",
    "        print(indent*3 + \"stdDevResponseTime =\", grp.stdDevResponseTime(txn), file=fi)\n",
    "        print(indent*3 + \"throughput =\", grp.throughput(txn), file=fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output location\n",
    "\n",
    "We can have the output go to stdout (in this case, the notebook itself) or to a file.  If you want the output to go to a file then modify the first line below (and modify it if you want to use a different file name or location).  Otherwise, comment the first line and uncomment the second line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fi = open(\"simout.txt\", \"w\")\n",
    "# fi = sys.stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random number generator seed\n",
    "\n",
    "We set the random number generator seed to have repeatable simulations.  Comment-out this line to have a different system-generated seed everytime the simulations are executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulations\n",
    "\n",
    "Several simulation scenarios are described and executed below.  Certain simulation parameters are varied for each scenario, but there are simulation parameters that are common to all scenarios.\n",
    "\n",
    "#### Common simulation parameters\n",
    "\n",
    "The following simulation parameters are common to all the simulations below and are embedded in the core simulation function:\n",
    "\n",
    "    # numUsers = 700\n",
    "    simtime = 200\n",
    "    hwThreads = 10\n",
    "    swThreads = 20\n",
    "    speed = 20\n",
    "    svc_1_comp_units = 2.0\n",
    "    svc_2_comp_units = 1.0\n",
    "    minThinkTime = 2.0 # .5 # 4\n",
    "    maxThinkTime = 10.0 # 1.5 # 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\n\\n@@@@@@@@@ Start comparative simulations @@@@@@@@@@\", file=fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res1 = deployment_example(numUsers=700, weight1=2, weight2=1, serverRange1=range(0, 10),\n",
    "                          serverRange2=range(0, 10))\n",
    "res2 = deployment_example(numUsers=700, weight1=2, weight2=1, serverRange1=range(0, 8), \n",
    "                          serverRange2=range(8, 10))\n",
    "\n",
    "print_results(fi=fi, **res1)\n",
    "print_results(fi=fi, **res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**conclusions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res1 = deployment_example(numUsers=700, weight1=5, weight2=1, serverRange1=range(0, 10),\n",
    "                          serverRange2=range(0, 10))\n",
    "res2 = deployment_example(numUsers=700, weight1=5, weight2=1, serverRange1=range(0, 8), \n",
    "                          serverRange2=range(8, 10))\n",
    "\n",
    "print_results(fi=fi, **res1)\n",
    "print_results(fi=fi, **res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**conclusions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res1 = deployment_example(numUsers=700, weight1=1, weight2=1, serverRange1=range(0, 10), \n",
    "                          serverRange2=range(0, 10))\n",
    "res2 = deployment_example(numUsers=700, weight1=1, weight2=1, serverRange1=range(0, 8), \n",
    "                          serverRange2=range(8, 10))\n",
    "\n",
    "print_results(fi=fi, **res1)\n",
    "print_results(fi=fi, **res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**conclusions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res1 = deployment_example(numUsers=700, weight1=1, weight2=1, serverRange1=range(0, 9), \n",
    "                          serverRange2=range(0, 9))\n",
    "res2 = deployment_example(numUsers=700, weight1=1, weight2=1, serverRange1=range(0, 7), \n",
    "                          serverRange2=range(7, 9))\n",
    "res3 = deployment_example(numUsers=700, weight1=1, weight2=1, serverRange1=range(0, 6),\n",
    "                          serverRange2=range(6, 9))\n",
    "\n",
    "print_results(fi=fi, **res1)\n",
    "print_results(fi=fi, **res2)\n",
    "print_results(fi=fi, **res3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**conclusions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vary the number of users over time\n",
    "\n",
    "The list below defines a step function the represents the number of users varying over time.  In this case, the number of users changes every 50 time units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usersCurve = [(0, 900), (50, 650), (100, 900), (150, 650)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res1 = deployment_example(numUsers=usersCurve, weight1=2, weight2=1, \n",
    "                          serverRange1=range(0, 10), serverRange2=range(0, 10))\n",
    "res2 = deployment_example(numUsers=usersCurve, weight1=2, weight2=1, \n",
    "                          serverRange1=range(0, 8), serverRange2=range(8, 10))\n",
    "\n",
    "print_results(fi=fi, **res1)\n",
    "print_results(fi=fi, **res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**conclusions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res1 = deployment_example(numUsers=usersCurve, weight1=1, weight2=1, \n",
    "                          serverRange1=range(0, 9), serverRange2=range(0, 9))\n",
    "res2 = deployment_example(numUsers=usersCurve, weight1=1, weight2=1, \n",
    "                          serverRange1=range(0, 7), serverRange2=range(7, 9))\n",
    "res3 = deployment_example(numUsers=usersCurve, weight1=1, weight2=1, \n",
    "                          serverRange1=range(0, 6), serverRange2=range(6, 9))\n",
    "\n",
    "print_results(fi=fi, **res1)\n",
    "print_results(fi=fi, **res2)\n",
    "print_results(fi=fi, **res3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**conclusions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n*** Done ***\", file=fi)\n",
    "\n",
    "if not fi == sys.stdout:\n",
    "    fi.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**overall conclusions**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
